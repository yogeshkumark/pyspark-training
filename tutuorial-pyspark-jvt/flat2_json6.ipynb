{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code \n",
    "\n",
    "# keyword symbol variable data \n",
    "\n",
    "def pyspark( dob ):\n",
    "    technology=\"bigdata\"\n",
    "    industry=\"banking finance\"\n",
    "    print(technology,industry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bigdata:\n",
    "    filesystem=\"haddop\"\n",
    "    coreeng=\"scala\"\n",
    "    def dis(self ):\n",
    "        print(self.filesystem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigdata banking finance\n",
      "haddop\n"
     ]
    }
   ],
   "source": [
    "# pyspark\n",
    "\n",
    "pyspark( 2010 )\n",
    "banking=bigdata()\n",
    "banking.dis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "json\n",
    "\n",
    "SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-CGB286I:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>jsonconversion</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1b0e5c68da0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html?highlight=sparksession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"jsonconversion\").getOrCreate()\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A cluster URL, namely, ‘local’ in these examples, which tells Spark how to connect to a cluster.\n",
    "\n",
    "# This ‘local’ is a special value that runs Spark on one thread on the local machine, without connecting to a cluster.\n",
    "\n",
    "# An application name, namely, ‘My App’ in these examples. This will identify your application on the cluster manager’s UI\n",
    "\n",
    "# if you connect to a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=jsonconversion, master=local[*]) created by getOrCreate at <ipython-input-14-53f545b28c8f>:3 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-21b11d0c9def>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'jsonconversion'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    339\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 341\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    342\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=jsonconversion, master=local[*]) created by getOrCreate at <ipython-input-14-53f545b28c8f>:3 "
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext('jsonconversion')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "txt = sc.textFile('C:/Users/jvt/PYSPARKTRAINING/file.txt')\n",
    "\n",
    "print(txt)\n",
    "\n",
    "print(txt.count())\n",
    "\n",
    "python_lines = txt.filter(lambda line: 'venkatesh' in line.lower())\n",
    "\n",
    "print(python_lines.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Category,StringType,true),StructField(Count,IntegerType,true),StructField(Description,StringType,true)))\n",
      "+--------+-----+-----------+\n",
      "|Category|Count|Description|\n",
      "+--------+-----+-----------+\n",
      "|    null| null|       null|\n",
      "|    null| null|       null|\n",
      "|    null| null|       null|\n",
      "+--------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType\n",
    "\n",
    "appName = \"PySpark Example - JSON file to Spark Data Frame\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(master) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a schema for the dataframe\n",
    "schema = StructType([\n",
    "    StructField('Category', StringType(), True),\n",
    "    StructField('Count', IntegerType(), True),\n",
    "    StructField('Description', StringType(), True)\n",
    "])\n",
    "\n",
    "# Create data frame\n",
    "json_file_path = 'cust_file.json'\n",
    "\n",
    "df = spark.read.json(json_file_path, schema, multiLine=True)\n",
    "\n",
    "print(df.schema)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(ID,StringType,true),StructField(name,IntegerType,true),StructField(DOB,StringType,true)))\n",
      "+----+----+----------+\n",
      "|  ID|name|       DOB|\n",
      "+----+----+----------+\n",
      "|1000|null|01/01/2000|\n",
      "+----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType\n",
    "\n",
    "appName = \"PySpark Example - JSON file to Spark Data Frame\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(master) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a schema for the dataframe\n",
    "schema = StructType([\n",
    "    StructField('ID', StringType(), True),\n",
    "    StructField('name', IntegerType(), True),\n",
    "    StructField('DOB', StringType(), True)\n",
    "])\n",
    "\n",
    "# Create data frame\n",
    "json_file_path = 'cust_file.json'\n",
    "\n",
    "df = spark.read.json(json_file_path, schema, multiLine=True)\n",
    "\n",
    "print(df.schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, name: string, DOB: string, Gender: string, Age : string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv(\"C:/Users/jvt/PYSPARKTRAINING/file.txt\",header=True,sep=\"|\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'ID'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+----------+------+----+\n",
      "|       ID|         name|       DOB|Gender|Age |\n",
      "+---------+-------------+----------+------+----+\n",
      "|     1000|  John Smith |01/01/2000|     F|  20|\n",
      "|     2000|Jim McDonald |02/02/2020|     M|  25|\n",
      "|       20| Jim McDonald|02/02/1999|     M|  25|\n",
      "|venkatesh|         null|      null|  null|null|\n",
      "+---------+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[36] at toJavaRDD at <unknown>:0\n"
     ]
    }
   ],
   "source": [
    "df_json = df.toJSON()        #JSON encoded string\n",
    "\n",
    "\n",
    "print(df_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"ID\":\"1000\",\"name\":\"John Smith \",\"DOB\":\"01/01/2000\",\"Gender\":\"F\",\"Age \":\"20\"}',\n",
       " '{\"ID\":\"2000\",\"name\":\"Jim McDonald \",\"DOB\":\"02/02/2020\",\"Gender\":\"M\",\"Age \":\"25\"}',\n",
       " '{\"ID\":\"20\",\"name\":\"Jim McDonald\",\"DOB\":\"02/02/1999\",\"Gender\":\"M\",\"Age \":\"25\"}',\n",
       " '{\"ID\":\"venkatesh\"}']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ID\":\"1000\",\"name\":\"John Smith \",\"DOB\":\"01/01/2000\",\"Gender\":\"F\",\"Age \":\"20\"}\n",
      "{\n",
      "{'ID': '1000', 'name': 'John Smith ', 'DOB': '01/01/2000', 'Gender': 'F', 'Age ': '20'}\n",
      "{\"ID\":\"2000\",\"name\":\"Jim McDonald \",\"DOB\":\"02/02/2020\",\"Gender\":\"M\",\"Age \":\"25\"}\n",
      "{\n",
      "{'ID': '2000', 'name': 'Jim McDonald ', 'DOB': '02/02/2020', 'Gender': 'M', 'Age ': '25'}\n",
      "{\"ID\":\"20\",\"name\":\"Jim McDonald\",\"DOB\":\"02/02/1999\",\"Gender\":\"M\",\"Age \":\"25\"}\n",
      "{\n",
      "{'ID': '20', 'name': 'Jim McDonald', 'DOB': '02/02/1999', 'Gender': 'M', 'Age ': '25'}\n",
      "{\"ID\":\"venkatesh\"}\n",
      "{\n",
      "{'ID': 'venkatesh'}\n"
     ]
    }
   ],
   "source": [
    "result =[]\n",
    "\n",
    "for row in df_json.collect():\n",
    "    print(row)\n",
    "    print(row[0])  # row['ID']\n",
    "    line = json.loads(row)\n",
    "    print(line)\n",
    "    result.append(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(result[0][\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ID': '1000', 'name': 'John Smith ', 'DOB': '01/01/2000', 'Gender': 'F', 'Age ': '20'}, {'ID': '2000', 'name': 'Jim McDonald ', 'DOB': '02/02/2020', 'Gender': 'M', 'Age ': '25'}, {'ID': '20', 'name': 'Jim McDonald', 'DOB': '02/02/1999', 'Gender': 'M', 'Age ': '25'}, {'ID': 'venkatesh'}]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/jvt/PYSPARKTRAINING/cust_file.json\",'w')as f:\n",
    "        f.write(json.dumps(result,indent=4))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='C:/Users/jvt/PYSPARKTRAINING/cust_file.json' mode='w' encoding='cp1252'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
