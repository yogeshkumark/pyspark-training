{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDDs are the main logical data units in Spark. They are a distributed collection of objects, \n",
    "# which are stored in memory or on disks of different machines of a cluster. A single RDD can be\n",
    "# divided into multiple logical partitions so that these partitions can be stored\n",
    "# and processed on different machines of a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resilience: RDDs track data lineage information to recover lost data, automatically on failure. \n",
    "# It is also called fault tolerance.\n",
    "\n",
    "# Distributed: Data present in an RDD resides on multiple nodes. It is distributed across different nodes of a cluster.\n",
    "\n",
    "# Lazy evaluation: Data does not get loaded in an RDD even if you define it. Transformations are actually computed \n",
    "# when you call an action, \n",
    "# such as count or collect, or save the output to a file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('James', 'Smith', 'USA', 'CA'), ('Michael', 'Rose', 'USA', 'NY')]\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(firstname='James', lastname='Smith', country='USA', state='CA')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('jvt').getOrCreate()\n",
    "\n",
    "# Dats structure 1 : List \n",
    "\n",
    "data = [  (\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "          (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "          (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "           (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "print ( data[0:2] )\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "\n",
    "# Dats structure 2 : Data frame \n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"firstname\",\"lastname\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.select(col(\"firstname\"),col(\"lastname\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+----------------------+-----+------+\n",
      "|name                  |state|gender|\n",
      "+----------------------+-----+------+\n",
      "|[James,, Smith]       |OH   |M     |\n",
      "|[Anna, Rose, ]        |NY   |F     |\n",
      "|[Julia, , Williams]   |OH   |F     |\n",
      "|[Maria, Anne, Jones]  |NY   |M     |\n",
      "|[Jen, Mary, Brown]    |NY   |M     |\n",
      "|[Mike, Mary, Williams]|OH   |M     |\n",
      "+----------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [\n",
    "        ( (\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
    "        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
    "        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
    "        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
    "        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
    "        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
    "        ]\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType \n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('name', StructType([\n",
    "         StructField('firstname', StringType(), True),\n",
    "         StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "         ])),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    "     ])\n",
    "\n",
    "\n",
    "df2 = spark.createDataFrame(data = data, schema = schema)\n",
    "\n",
    "df2.printSchema()\n",
    "\n",
    "df2.show(truncate=False) # shows all columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|name                  |\n",
      "+----------------------+\n",
      "|[James,, Smith]       |\n",
      "|[Anna, Rose, ]        |\n",
      "|[Julia, , Williams]   |\n",
      "|[Maria, Anne, Jones]  |\n",
      "|[Jen, Mary, Brown]    |\n",
      "|[Mike, Mary, Williams]|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"name\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|James    |Smith   |\n",
      "|Anna     |        |\n",
      "|Julia    |Williams|\n",
      "|Maria    |Jones   |\n",
      "|Jen      |Brown   |\n",
      "|Mike     |Williams|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"name.firstname\",\"name.lastname\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+\n",
      "|firstname|middlename|lastname|\n",
      "+---------+----------+--------+\n",
      "|James    |null      |Smith   |\n",
      "|Anna     |Rose      |        |\n",
      "|Julia    |          |Williams|\n",
      "|Maria    |Anne      |Jones   |\n",
      "|Jen      |Mary      |Brown   |\n",
      "|Mike     |Mary      |Williams|\n",
      "+---------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"name.*\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map()\tReturns a new RDD by applying the function on each data element\n",
    "\n",
    "filter()\tReturns a new RDD formed by selecting those elements of the source on which the function returns true\n",
    "\n",
    "reduceByKey()\tAggregates the values of a key using a function\n",
    "\n",
    "groupByKey()\tConverts a (key, value) pair into a (key, <iterable value>) pair\n",
    "\n",
    "union()\tReturns a new RDD that contains all elements and arguments from the source RDD\n",
    "\n",
    "intersection()\tReturns a new RDD that contains an intersection of the elements in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count()\tGets the number of data elements in an RDD\n",
    "\n",
    "collect()\tGets all the data elements in an RDD as an array\n",
    "\n",
    "reduce()\tAggregates data elements into an RDD by taking two arguments and returning one\n",
    "\n",
    "take(n)\tFetches the first n elements of an RDD\n",
    "\n",
    "foreach(operation)\tExecutes the operation for each data element in an RDD\n",
    "\n",
    "first()\tRetrieves the first data element of an RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- jobStartDate: string (nullable = true)\n",
      " |-- isGraduated: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n",
      "+---------+---+------------+-----------+------+------+\n",
      "|firstname|age|jobStartDate|isGraduated|gender|salary|\n",
      "+---------+---+------------+-----------+------+------+\n",
      "|James    |34 |2006-01-01  |true       |M     |3000.6|\n",
      "|Michael  |33 |1980-01-10  |true       |F     |3300.8|\n",
      "|Robert   |37 |06-01-1992  |false      |M     |5000.5|\n",
      "+---------+---+------------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('gogetit').getOrCreate()\n",
    "\n",
    "simpleData = [(\"James\",34,\"2006-01-01\",\"true\",\"M\",3000.60),\n",
    "    (\"Michael\",33,\"1980-01-10\",\"true\",\"F\",3300.80),\n",
    "    (\"Robert\",37,\"06-01-1992\",\"false\",\"M\",5000.50)\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"age\",\"jobStartDate\",\"isGraduated\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- jobStartDate: date (nullable = true)\n",
      " |-- isGraduated: boolean (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- isGraduated: string (nullable = true)\n",
      " |-- jobStartDate: string (nullable = true)\n",
      "\n",
      "+---+-----------+------------+\n",
      "|age|isGraduated|jobStartDate|\n",
      "+---+-----------+------------+\n",
      "|34 |true       |2006-01-01  |\n",
      "|33 |true       |1980-01-10  |\n",
      "|37 |false      |null        |\n",
      "+---+-----------+------------+\n",
      "\n",
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- isGraduated: boolean (nullable = true)\n",
      " |-- jobStartDate: date (nullable = true)\n",
      "\n",
      "+---+-----------+------------+\n",
      "|age|isGraduated|jobStartDate|\n",
      "+---+-----------+------------+\n",
      "|34 |true       |2006-01-01  |\n",
      "|33 |true       |1980-01-10  |\n",
      "|37 |false      |null        |\n",
      "+---+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType\n",
    "\n",
    "df2 = df.withColumn(\"age\",col(\"age\").cast(StringType())) \\\n",
    "    .withColumn(\"isGraduated\",col(\"isGraduated\").cast(BooleanType())) \\\n",
    "    .withColumn(\"jobStartDate\",col(\"jobStartDate\").cast(DateType()))\n",
    "df2.printSchema()\n",
    "\n",
    "df3 = df2.selectExpr(\"cast(age as int) age\",\n",
    "    \"cast(isGraduated as string) isGraduated\",\n",
    "    \"cast(jobStartDate as string) jobStartDate\")\n",
    "df3.printSchema()\n",
    "df3.show(truncate=False)\n",
    "\n",
    "df3.createOrReplaceTempView(\"CastExample\")\n",
    "df4 = spark.sql(\"SELECT STRING(age),BOOLEAN(isGraduated),DATE(jobStartDate) from CastExample\")\n",
    "df4.printSchema()\n",
    "df4.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".ArrayType()\n",
    ".IntegerType()\n",
    ".LongType()\n",
    ".StructField()\n",
    ".StructType()\n",
    ".StringType()\n",
    ".DoubleType()\n",
    ".Row()\n",
    ".FloatType()\n",
    ".BinaryType()\n",
    ".TimestampType()\n",
    ".DataType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------+----+\n",
      "|  ID|name| DOB|Gender| Age|\n",
      "+----+----+----+------+----+\n",
      "|null|null|null|  null|null|\n",
      "+----+----+----+------+----+\n",
      "\n",
      "+---+----+---+------+---+---+----+---+------+---+\n",
      "| ID|name|DOB|Gender|Age| ID|name|DOB|Gender|Age|\n",
      "+---+----+---+------+---+---+----+---+------+---+\n",
      "+---+----+---+------+---+---+----+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType\n",
    "\n",
    "appName = \"PySpark Example - JSON file to Spark Data Frame\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(master) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('ID', IntegerType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('DOB', StringType(), True),\n",
    "    StructField('Gender', StringType(), True),\n",
    "    StructField('Age', IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "json_file_path = 'C:/Users/jvt/PYSPARKTRAINING/24septsolutions/file.txt'\n",
    "json_file_path1 = 'C:/Users/jvt/PYSPARKTRAINING/24septsolutions/jsaonoutput.json'\n",
    "\n",
    "df1 = spark.read.json(json_file_path, schema, multiLine=True)\n",
    "df1.show(4)\n",
    "\n",
    "\n",
    "df2 = spark.read.json(json_file_path1, schema, multiLine=True)\n",
    "d=df1.join(df2, df1.ID == df2.ID).filter(df1.ID==1000)\n",
    "\n",
    "d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[ID: string, name: string, DOB: string, Gender: string, Age: string]\n",
      "MapPartitionsRDD[110] at toJavaRDD at <unknown>:0\n",
      "John Smith  20\n",
      "Jim McDonald  25\n",
      "Jim McDonald 25\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType\n",
    "\n",
    "appName = \"PySpark Example - JSON file to Spark Data Frame\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(master) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('ID', StringType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('DOB', StringType(), True),\n",
    "    StructField('Gender', StringType(), True),\n",
    "    StructField('Age', StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "json_file_path = 'C:/Users/jvt/PYSPARKTRAINING/24septsolutions/data.json'\n",
    "df1 = spark.read.json(json_file_path, schema, multiLine=True)\n",
    "print(df1)\n",
    "# data = json.load(df1)\n",
    "df_json = df1.toJSON()\n",
    "print(df_json)\n",
    "\n",
    "for row in df1.rdd.collect():\n",
    "    print(row.name,row.Age)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "user_row = namedtuple('user_row', 'dob age is_fan'.split())\n",
    "\n",
    "data = [\n",
    "    user_row('1990-05-03', 29, True),\n",
    "    user_row('1994-09-23', 25, False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.user_row"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[user_row(dob='1990-05-03', age=29, is_fan=True),\n",
       " user_row(dob='1994-09-23', age=25, is_fan=False)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "sc = SparkContext('local')\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dob: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- is_fan: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_df = spark.createDataFrame(data)\n",
    "\n",
    "user_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+\n",
      "|       dob|age|is_fan|\n",
      "+----------+---+------+\n",
      "|1990-05-03| 29|  true|\n",
      "|1994-09-23| 25| false|\n",
      "+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jvt\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    }
   ],
   "source": [
    "data_list = [\n",
    "    ('1990-05-03', 29, True),\n",
    "    ('1994-09-23', 25, False)\n",
    "]\n",
    "\n",
    "data = [ {'dob': r[0], 'age': r[1], 'is_fan': r[2]} for r in data_list  ]\n",
    "\n",
    "user_df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, dob: string, is_fan: boolean]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- is_fan: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+\n",
      "|age|       dob|is_fan|\n",
      "+---+----------+------+\n",
      "| 29|1990-05-03|  true|\n",
      "| 25|1994-09-23| false|\n",
      "+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "user_row = Row(\"dob\", \"age\", \"is_fan\")\n",
    "\n",
    "data = [\n",
    "    user_row('1990-05-03', 29, True),\n",
    "    user_row('1994-09-23', 25, False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dob='1990-05-03', age=29, is_fan=True),\n",
       " Row(dob='1994-09-23', age=25, is_fan=False)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+\n",
      "|       dob|age|is_fan|\n",
      "+----------+---+------+\n",
      "|1990-05-03| 29|  true|\n",
      "|1994-09-23| 25| false|\n",
      "+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df = spark.createDataFrame(data)\n",
    "user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+\n",
      "|       dob|age|is_fan|\n",
      "+----------+---+------+\n",
      "|1990-05-03| 29|  true|\n",
      "|1994-09-23| 25| false|\n",
      "+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ('1990-05-03', 29, True),\n",
    "    ('1994-09-23', 25, False)\n",
    "]\n",
    "df = spark.createDataFrame(data, ['dob', 'age', 'is_fan'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+\n",
      "|       dob|age|is_fan|\n",
      "+----------+---+------+\n",
      "|1990-05-03| 29|  true|\n",
      "|1994-09-23| 25| false|\n",
      "+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.types as st\n",
    "\n",
    "data = [\n",
    "    ('1990-05-03', 29, True),\n",
    "    ('1994-09-23', 25, False)\n",
    "]\n",
    "\n",
    "user_schema = st.StructType([\n",
    "    st.StructField('dob', st.StringType(), True),\n",
    "    st.StructField('age', st.IntegerType(), True),\n",
    "    st.StructField('is_fan', st.BooleanType(), True)\n",
    "])\n",
    "\n",
    "user_df = spark.createDataFrame(data, user_schema)\n",
    "user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+\n",
      "|       dob|age|is_fan|\n",
      "+----------+---+------+\n",
      "|1990-05-03| 29|  true|\n",
      "|1994-09-23| 25| false|\n",
      "+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ('1990-05-03', 29, True),\n",
    "    ('1994-09-23', 25, False)\n",
    "]\n",
    "\n",
    "user_schema = \"dob:string, age:int, is_fan: boolean\"\n",
    "\n",
    "user_df = spark.createDataFrame(data, user_schema)\n",
    "user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>petalLength</th>\n",
       "      <th>petalWidth</th>\n",
       "      <th>sepalLength</th>\n",
       "      <th>sepalWidth</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   petalLength  petalWidth  sepalLength  sepalWidth species\n",
       "0          1.4         0.2          5.1         3.5  setosa\n",
       "1          1.4         0.2          4.9         3.0  setosa"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris_data = pd.read_json(\"https://raw.githubusercontent.com/domoritz/maps/master/data/iris.json\")\n",
    "\n",
    "iris_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'struct', 'fields': [{'name': 'dob', 'type': 'string', 'nullable': True, 'metadata': {}}, {'name': 'age', 'type': 'integer', 'nullable': True, 'metadata': {}}, {'name': 'is_fan', 'type': 'boolean', 'nullable': True, 'metadata': {}}]}\n",
      "+----------+---+------+\n",
      "|       dob|age|is_fan|\n",
      "+----------+---+------+\n",
      "|1990-05-03| 29|  true|\n",
      "|1994-09-23| 25| false|\n",
      "+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pyspark.sql.types as st\n",
    "\n",
    "schema_json_str = \"\"\"\n",
    "{\n",
    "  \"type\": \"struct\",\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"name\": \"dob\",\n",
    "      \"type\": \"string\",\n",
    "      \"nullable\": true,\n",
    "      \"metadata\": {}\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"age\",\n",
    "      \"type\": \"integer\",\n",
    "      \"nullable\": true,\n",
    "      \"metadata\": {}\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"is_fan\",\n",
    "      \"type\": \"boolean\",\n",
    "      \"nullable\": true,\n",
    "      \"metadata\": {}\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Parse JSON string into python dictionary\n",
    "schema_dict = json.loads(schema_json_str)\n",
    "\n",
    "print(schema_dict)\n",
    "\n",
    "# Create StructType from python dictionary\n",
    "schema = st.StructType.fromJson(schema_dict)\n",
    "\n",
    "\n",
    "data = [\n",
    "    ('1990-05-03', 29, True),\n",
    "    ('1994-09-23', 25, False)\n",
    "]\n",
    "\n",
    "user_df = spark.createDataFrame(data, schema)\n",
    "user_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
